{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed42e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8accc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msdis_lidar.shapefile_handler import ShapefileHandler\n",
    "from msdis_lidar.downloader import LidarDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01536464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msdis_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410847d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(LidarDownloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c32903",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = ShapefileHandler(\"Perche_Creek_HU12.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2f5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> geometries found\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m10\u001b[0m geometries found\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CRS = EPSG:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3857</span> is not compatible\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CRS = EPSG:\u001b[1;36m3857\u001b[0m is not compatible\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Projecting to EPSG:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4326</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Projecting to EPSG:\u001b[1;36m4326\u001b[0m \u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Simplifying the polygons<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Simplifying the polygons\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf164eb14aa4f509f7e549450375461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polys = handler.get_polygon_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468bfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = LidarDownloader(\"ddd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356486f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'GeoDataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6f6941649acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0murl_gdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_lidar_download_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mjwd1\\OneDrive\\Documents\\GitHub\\msdis_lidar\\msdis_lidar\\downloader.py\u001b[0m in \u001b[0;36mfetch_lidar_download_links\u001b[1;34m(self, polygons)\u001b[0m\n\u001b[0;32m     97\u001b[0m                             \u001b[1;31m# Append the new data to the existing GeoDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgdf_lidar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                                 \u001b[0mgdf_lidar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgdf_lidar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                                 \u001b[0mgdf_lidar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6301\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GeoDataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "url_gdf = downloader.fetch_lidar_download_links(polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718260a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_info(info_gdf, i):\n",
    "    # i is the tile index in the LiDAR infor geodatabase \n",
    "    # this geodatbase is the output of running fetch_lidar_download_links tool for an input polygon\n",
    "    name =  info_gdf.iloc[i]['ftp_1'].split('/')[-1]\n",
    "    url =  info_gdf.iloc[i]['updated_ft']\n",
    "    \n",
    "    return name, url    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fdf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import track, Progress, BarColumn, TextColumn\n",
    "from rich.console import Console\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "from rich import print\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8d85aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3fadc2a7558b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tile_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_gdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m78\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url_gdf' is not defined"
     ]
    }
   ],
   "source": [
    "name, url = get_tile_info(url_gdf, 78)\n",
    "print(name, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(url, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fead02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, name):\n",
    "    local_filename = name\n",
    "    \n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b930f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, name):   \n",
    "    # Step 1: Download the file\n",
    "    zip_path = download_file(url, name)\n",
    "    # Step 2: Check if the downloaded file is a zip file\n",
    "    if zipfile.is_zipfile(name):\n",
    "        # Step 3: Unzip the file\n",
    "#         print(f'unzipping the file...', end = '\\r')\n",
    "        unzip_file(name)\n",
    "        \n",
    "        # Step 4: Delete the original zip file\n",
    "        os.remove(name)\n",
    "        \n",
    "        las_file = name[:-4]\n",
    "\n",
    "    else:\n",
    "#         print(f\"{zip_path} is not a zip file, proceesing with las file name\", end = '\\r')\n",
    "        las_file = name\n",
    "        \n",
    "    return(las_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ca1de",
   "metadata": {},
   "source": [
    "### Minimum Sampling Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30304c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_las_info(las_file):\n",
    "    las = laspy.read(las_file)\n",
    "    \n",
    "    # extracting the ground points from las file\n",
    "    classifications = las.classification\n",
    "    ground_points = las.points[classifications==2]\n",
    "    x = ground_points.x\n",
    "    y = ground_points.y\n",
    "    z = ground_points.z  \n",
    "    \n",
    "    # Define the bounding box for random centroid locations (adjust as needed)\n",
    "    min_x, max_x = np.min(x),np.max(x)\n",
    "    min_y, max_y = np.min(y),np.max(y)\n",
    "    \n",
    "    A = (max_x - min_x ) * (max_y - min_y)\n",
    "    return x, y, z, min_x, max_x, min_y, max_y, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17778dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, min_x, max_x, min_y, max_y, A = get_las_info(las_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_samples(A, L1 =300, L2 = 1000, Pc = 0.95):\n",
    "    # A = area of the las tile in square feet\n",
    "    # Pc = confidence level from 0 to 1.\n",
    "    P0 = 1-(2*L1*L2/(np.pi*A))\n",
    "    print(P0)\n",
    "    if P0<0:\n",
    "        num_of_lines = int(5.0)\n",
    "    else:\n",
    "        num_of_lines = int(np.ceil(np.log(1-Pc)/np.log(P0)))\n",
    "    \n",
    "    print(f' {num_of_lines} sampling line is needed for {Pc*100}% confidence level')\n",
    "    return num_of_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples(A, L2=1000, Pc=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laspy\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_lines(num_of_lines, min_x, max_x, min_y, max_y, line_length = 300):\n",
    "    \n",
    "    # Create an empty GeoDataFrame to store the random lines\n",
    "    gdf = gpd.GeoDataFrame(columns=['geometry'])\n",
    "\n",
    "\n",
    "    # Generate random lines\n",
    "    for _ in range(num_of_lines):\n",
    "        centroid_x = random.uniform(min_x, max_x)\n",
    "        centroid_y = random.uniform(min_y, max_y)\n",
    "        \n",
    "        orientation_angle_degrees = random.uniform(0, 360)\n",
    "\n",
    "        orientation_angle_radians = orientation_angle_degrees * (3.14159265359 / 180.0)\n",
    "\n",
    "        line_endpoint_x1 = centroid_x - (line_length/2)*np.cos(orientation_angle_radians)\n",
    "        line_endpoint_y1 = centroid_y - (line_length/2)*np.sin(orientation_angle_radians)\n",
    "        line_endpoint_x2 = centroid_x + (line_length/2)*np.cos(orientation_angle_radians)\n",
    "        line_endpoint_y2 = centroid_y + (line_length/2)*np.sin(orientation_angle_radians)\n",
    "\n",
    "        line = LineString([(line_endpoint_x1, line_endpoint_y1), (line_endpoint_x2, line_endpoint_y2)])\n",
    "\n",
    "        gdf = gdf.append({'geometry': line}, ignore_index=True)\n",
    "        \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dab36e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gdf_lidar = gpd.GeoDataFrame(columns=['geometry', 'elv_data'])   \n",
    "    \n",
    "# for tile_no in range(len(url_gdf)):\n",
    "#     name, url = get_tile_info (url_gdf, tile_no)\n",
    "    \n",
    "#     print(f'\\rdownloading tile # {tile_no} out of {len(url_gdf)}...', end = '', flush = True)\n",
    "#     las_file = main(url, name)\n",
    "#     print(f'\\r             {las_file} downloaded successfully.', end = '')\n",
    "\n",
    "#     #getting las file info\n",
    "#     print(f'\\rgetting las file information for LiDAR tile {las_file}...', end = '')\n",
    "#     x, y, z, min_x, max_x, min_y, max_y, A = get_las_info(las_file)\n",
    "#     print(f'\\r                      information extracted susscessfully for tile {las_file}.', end = '')\n",
    "\n",
    "    \n",
    "#     # how many sampling line we might nee\n",
    "#     num_of_lines = min_samples(A, L2 = 1250, Pc = 0.95)\n",
    "    \n",
    "    \n",
    "#     # extracting sampling lines\n",
    "#     gdf = get_sample_lines(num_of_lines, min_x, max_x, min_y, max_y, line_length = 300)\n",
    "    \n",
    "#     print(f'\\rextracting cross section elevation data for sample lines...', end = '')\n",
    "#     for i in range(num_of_lines):\n",
    "#         cs_data, cs_line = get_cross_section_data(gdf, i, x, y, z)\n",
    "#         Delta_ELV = np.max(cs_data.T[2])-np.min(cs_data.T[2])\n",
    "#         if Delta_ELV > 2.0:\n",
    "#             gdf_lidar = gdf_lidar.append({'geometry': cs_line, 'elv_data':cs_data}, ignore_index=True)\n",
    "        \n",
    "#     print(f'\\r                      elevation data extracted successfully for tile {tile_no+1}/{len(url_gdf)}', end='')\n",
    "\n",
    "    \n",
    "#     # Delete the original las file\n",
    "#     os.remove(las_file)\n",
    "#     print(f'\\r{las_file} removed.', end = '')\n",
    "#     print(f'\\r--------------------------------------------------------------------------------------------', end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(progress.add_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(progress.update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879c799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rich.progress import Progress, track\n",
    "from rich.console import Console\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Initialize console for rich logging\n",
    "console = Console()\n",
    "\n",
    "# Initialize an empty GeoDataFrame\n",
    "# gdf_lidar = gpd.GeoDataFrame(columns=['geometry', 'elv_data'])\n",
    "\n",
    "# Using rich Progress for better progress tracking\n",
    "with Progress(\n",
    "    TextColumn(\"[progress.description]{task.description}\"),\n",
    "    BarColumn(),\n",
    "    TextColumn(\"[progress.percentage]{task.percentage:>3.2f}%\"),  # Default precision\n",
    "    console=console\n",
    "    \n",
    ") as progress:\n",
    "    task = progress.add_task(\"Downloading tiles...\", total=len(url_gdf)-768)\n",
    "\n",
    "    for tile_no in range(1171,len(url_gdf)):\n",
    "        name, url = get_tile_info(url_gdf, tile_no)\n",
    "        progress.update(task, description=f\"Downloading tile # {tile_no + 1} of {len(url_gdf)}\", advance=1)\n",
    "        las_file = main(url, name)\n",
    "        console.log(f\"[bold green]{las_file} downloaded successfully.[/bold green]\")\n",
    "\n",
    "        # Getting las file info\n",
    "        console.log(f\"Getting las file information for LiDAR tile {las_file}...\")\n",
    "        x, y, z, min_x, max_x, min_y, max_y, A = get_las_info(las_file)\n",
    "        console.log(f\"[bold green]Information extracted successfully for tile {las_file}.[/bold green]\")\n",
    "\n",
    "        # Determine how many sampling lines are needed\n",
    "        num_of_lines = min_samples(A, L2=1000, Pc=0.95)\n",
    "        \n",
    "        # Extracting sampling lines\n",
    "        console.log(\"Extracting cross section elevation data for sample lines...\")\n",
    "        gdf = get_sample_lines(num_of_lines, min_x, max_x, min_y, max_y, line_length=300)\n",
    "        \n",
    "        for i in range(num_of_lines):\n",
    "            cs_data, cs_line = get_cross_section_data(gdf, i, x, y, z)\n",
    "            if cs_data.size ==0: \n",
    "                print(\"nothing to add, no data here.\")\n",
    "            else:\n",
    "                Delta_ELV = np.max(cs_data.T[2])-np.min(cs_data.T[2])\n",
    "                if Delta_ELV > 2.0:\n",
    "                    gdf_lidar = gdf_lidar.append({'geometry': cs_line, 'elv_data': cs_data}, ignore_index=True)\n",
    "                else:\n",
    "                    print(\"nothing to add, too flat of a place\")\n",
    "        \n",
    "        console.log(f\"[bold green]Elevation data extracted successfully for tile {tile_no + 1}/{len(url_gdf)}.[/bold green]\")\n",
    "\n",
    "        # Delete the original las file\n",
    "        os.remove(las_file)\n",
    "        console.log(f\"[red]{las_file} removed.[/red]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c505a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "las_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41982bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_usage = gdf_lidar.memory_usage(deep=True)\n",
    "print(memory_usage)\n",
    "\n",
    "total_memory = memory_usage.sum()\n",
    "print(f\"Total memory usage: {total_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4938259",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = get_sample_lines(num_of_lines, min_x, max_x, min_y, max_y)\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f61372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_section_data(gdf, i, x, y, z):\n",
    "       \n",
    "    cross_section_line = gdf[\"geometry\"][i]\n",
    "    # line start and end point \n",
    "    x1 = cross_section_line.coords[0][0]\n",
    "    y1 = cross_section_line.coords[0][1]\n",
    "    x2 = cross_section_line.coords[1][0]\n",
    "    y2 = cross_section_line.coords[1][1]\n",
    "\n",
    "\n",
    "    # having three points on a plane we can define the plane\n",
    "    # as we have 2 points in 2D space we assume a third point in \n",
    "    # an arbitray elevation z = 5. This will enforce a vertical plane \n",
    "    # passes throght our line. \n",
    "    P0 = np.array([x1, y1, 0])\n",
    "    P1 = np.array([x1, y1, 5])\n",
    "    P2 = np.array([x2, y2, 0])\n",
    "\n",
    "    # We can find its normal vector by applying cross product rule. \n",
    "    N = np.cross(P1-P0, P2-P1)\n",
    "    unit_N = N/np.linalg.norm(N)  # unit normal vector \n",
    "\n",
    "    # stacking all the points from the LAS file \n",
    "    points = np.vstack([x, y, z]).T\n",
    "\n",
    "    # choosing on point on our sampling line\n",
    "    plane_point = np.array([x2, y2, 0.0])  # Example point on the plane\n",
    "\n",
    "    # vector from each elevation point to starting point on the sampling line. \n",
    "    V = points - plane_point\n",
    "\n",
    "    # The distance between each elevation point and the verticl plane passing from our sampling line \n",
    "    distance = np.dot(V, unit_N)\n",
    "\n",
    "    # to what distance of sampling plane we want to get elevation points \n",
    "    thickness = 6.0  # Cross section thickness\n",
    "\n",
    "\n",
    "    # Extract cross section points\n",
    "    cross_section_points = points[np.abs(distance) < thickness/2]\n",
    "\n",
    "\n",
    "    # this section filters points that are between the start and end point of the sampling line\n",
    "    def is_between(point, P2, P0 ):\n",
    "        D = P2- P0  # direction of normal vector is from P2 to P0\n",
    "        unit_D = D/np.linalg.norm(D)\n",
    "        V2 = point - P2    # distance to point on start plane\n",
    "        V0 = point - P0    # distance to point on end plane\n",
    "\n",
    "        d2 = np.dot(V2, unit_D)  # prependicular distance to start plane\n",
    "        d0 = np.dot(V0, unit_D)  # prependicular distance to end plane\n",
    "\n",
    "        return(d0>=0 and d2<=0)\n",
    "\n",
    "    #using the filter we defined above\n",
    "    cross_section_points = np.array([point for point in cross_section_points if is_between(point, P2, P0)])    \n",
    "    return cross_section_points, cross_section_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lidar = gpd.GeoDataFrame(columns=['geometry', 'elv_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2edfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in all_the_tiles:\n",
    "#     create an empty geodataframe to store the sampling line and thier elevation sequence and the centroid of each line \n",
    "#     download i\n",
    "#     if i is zip\n",
    "#     unzip i \n",
    "#     get the area\n",
    "#     get the crs\n",
    "#     calcualte the number of sampling lines\n",
    "#     generating randomly located and randomly oriented lines \n",
    "#     extract the elevation data for each sampling line to the geodataframe created earlier\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saviong the data\n",
    "gdf_lidar.to_csv(\"LiDAR_data_Perche_Creek.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d3fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
